{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c358a888",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a4657",
   "metadata": {},
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407aacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Version: 3.4.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print('spaCy Version: %s' % (spacy.__version__))\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564c3b9",
   "metadata": {},
   "source": [
    "Check pre-defined stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1926fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['its', 'another', 'thereafter', 'be', 'anything', 'enough', 'if', 'throughout', 'part', 'not']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3dff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'Original Article: In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removin/g these stop words to support phrase search.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10528",
   "metadata": {},
   "source": [
    "Step 4:Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5a6e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Original Article: In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removin/g these stop words to support phrase search.\n",
      "\n",
      "['Original', 'Article', ':', 'computing', ',', 'stop', 'words', 'words', 'filtered', 'processing', 'natural', 'language', 'data', '(', 'text).[1', ']', '\"', 'stop', 'words', '\"', 'usually', 'refers', 'common', 'words', 'language', ',', 'single', 'universal', 'list', 'stop', 'words', 'natural', 'language', 'processing', 'tools', ',', 'tools', 'use', 'list', '.', 'tools', 'specifically', 'avoid', 'removin', '/', 'g', 'stop', 'words', 'support', 'phrase', 'search', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = spacy_nlp(article)\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794f674",
   "metadata": {},
   "source": [
    "Step 5: Add customize stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1373c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Original Article: In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removin/g these stop words to support phrase search.\n",
      "\n",
      "['Original', 'Article', ':', ',', 'stop', 'words', 'words', 'processing', 'natural', 'data', '(', 'text).[1', ']', '\"', 'stop', 'words', '\"', 'usually', 'refers', 'common', 'words', ',', 'single', 'universal', 'list', 'stop', 'words', 'natural', 'processing', 'tools', ',', 'tools', 'use', 'list', '.', 'tools', 'specifically', 'avoid', 'removin', '/', 'g', 'stop', 'words', 'support', 'phrase', 'search', '.']\n"
     ]
    }
   ],
   "source": [
    "customize_stop_words = [\n",
    "    'computing', 'filtered','language'\n",
    "]\n",
    "for w in customize_stop_words:\n",
    "    spacy_nlp.vocab[w].is_stop = True\n",
    "doc = spacy_nlp(article)\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b128fa4",
   "metadata": {},
   "source": [
    "After added “computing” and “filtered”, it will be removed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1cefb",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d4624",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7931c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Version: 3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RAHUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "print('NLTK Version: %s' % (nltk.__version__))\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ccdf3",
   "metadata": {},
   "source": [
    "Step 3: Check pre-defined stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c8f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 179\n",
      "First ten stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "print('Number of stop words: %d' % len(nltk_stopwords))\n",
    "print('First ten stop words: %s' % list(nltk_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2879062",
   "metadata": {},
   "source": [
    "Step 4: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a8eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAHUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bafed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Original Article: In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removin/g these stop words to support phrase search.\n",
      "\n",
      "['Original', 'Article', ':', 'In', 'computing', ',', 'stop', 'words', 'words', 'filtered', 'processing', 'natural', 'language', 'data', '(', 'text', ')', '.', '[', '1', ']', 'Though', '``', 'stop', 'words', \"''\", 'usually', 'refers', 'common', 'words', 'language', ',', 'single', 'universal', 'list', 'stop', 'words', 'used', 'natural', 'language', 'processing', 'tools', ',', 'indeed', 'tools', 'even', 'use', 'list', '.', 'Some', 'tools', 'specifically', 'avoid', 'removin/g', 'stop', 'words', 'support', 'phrase', 'search', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(article)\n",
    "tokens = [token for token in tokens if not token in nltk_stopwords]\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c3efd",
   "metadata": {},
   "source": [
    "General words such as “are”, “the” are removed as well. For example, “indeed” is removed in NLTK but not spaCy. On the other hand, “used” are removed in spaCy but not NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d662e08",
   "metadata": {},
   "source": [
    "# jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a67a35",
   "metadata": {},
   "source": [
    "For Chinese word, we use the similar ideas to filter out words if it is stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bdc033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in c:\\users\\rahul\\.conda\\envs\\ic\\lib\\site-packages (0.42.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3dba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba Version: 0.42.1\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "print('jieba Version: %s' % jieba.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50f9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba_stop_words = [\n",
    "    '的', '了', '和', '是', '就', '都', '而', '及', '與', \n",
    "    '著', '或', '一個', '沒有', '我們', '你們', '妳們', \n",
    "    '他們', '她們', '是否'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71212c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article2 = ' 在信息檢索中，為節省存儲空間和提高搜索效率，在處理自然語言數據（或文本）之前或之後會自動過濾掉某些字或詞，這些字或詞即被稱為Stop Words(停用詞)。不要把停用詞與安全口令混淆。 這些停用詞都是人工輸入、非自動化生成的，生成後的停用詞會形成一個停用詞表。但是，並沒有一個明確的停用詞表能夠適用於所有的工具。甚至有一些工具是明確地避免使用停用詞來支持短語搜索的。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2d2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article:  在信息檢索中，為節省存儲空間和提高搜索效率，在處理自然語言數據（或文本）之前或之後會自動過濾掉某些字或詞，這些字或詞即被稱為Stop Words(停用詞)。不要把停用詞與安全口令混淆。 這些停用詞都是人工輸入、非自動化生成的，生成後的停用詞會形成一個停用詞表。但是，並沒有一個明確的停用詞表能夠適用於所有的工具。甚至有一些工具是明確地避免使用停用詞來支持短語搜索的。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\RAHUL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.599 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '在', '信息', '檢索', '中', '，', '為節', '省存', '儲空間', '提高', '搜索', '效率', '，', '在', '處理', '自然', '語言數', '據', '（', '文本', '）', '之前', '之後會', '自動', '過濾', '掉', '某些', '字', '詞', '，', '這些', '字', '詞', '即', '被', '稱', '為', 'Stop', ' ', 'Words', '(', '停用', '詞', ')', '。', '不要', '把', '停用', '詞', '安全', '口令', '混淆', '。', ' ', '這些', '停用', '詞', '人工', '輸入', '、', '非自動', '化生成', '，', '生成', '後', '停用', '詞會', '形成', '停用', '詞表', '。', '但是', '，', '並沒有', '明確', '停用', '詞表能夠', '適用', '於', '所有', '工具', '。', '甚至', '有', '一些', '工具', '明確', '地', '避免', '使用', '停用', '詞來', '支持', '短語', '搜索', '。']\n"
     ]
    }
   ],
   "source": [
    "print('Original Article: %s' % (article2))\n",
    "print()\n",
    "words = jieba.cut(article2, cut_all=False)\n",
    "words = [str(word) for word in words if not str(word) in jieba_stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac34e77",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "The procedure of removing stop words is similar across libraries so the most importance is defining your own stop words. In initial phase, pre-defined stop words can be adopted but more and more words should be added into stop word list later on.\n",
    "\n",
    "So besides, using spaCy or NLTK pre-defined stop words, we can use other words which are defined by other party such as Stanford NLP and Rank NL. You may check out the stop list from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30333ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
