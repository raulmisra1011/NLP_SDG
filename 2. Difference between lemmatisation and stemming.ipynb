{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a877ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8626ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RAHUL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f5389",
   "metadata": {},
   "source": [
    "PorterStemmer: One of the most commonly used stemmers. It is based on Porter Stemming Algorithm.  \n",
    "\n",
    "\n",
    "LancasterStemmer: It is based on Lancaster Stemming Algorithm and can sometimes result in more aggressive stemming than PorterStemmer.\n",
    "\n",
    "\n",
    "WordNetLemmatiser: It lemmatises using WordNet lexical database. Returns the input word unchanged if it cannot be found in WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de354815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "# Instantiate stemmers and lemmatiser\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "# Create function that normalises text using all three techniques\n",
    "def normalise_text(words, pos='v'):\n",
    "    \"\"\"Stem and lemmatise each word in a list. Return output in a dataframe.\"\"\"\n",
    "    normalised_text = pd.DataFrame(index=words, columns=['Porter', 'Lancaster', 'Lemmatiser'])\n",
    "    for word in words:\n",
    "        normalised_text.loc[word,'Porter'] = porter.stem(word)\n",
    "        normalised_text.loc[word,'Lancaster'] = lancaster.stem(word)\n",
    "        normalised_text.loc[word,'Lemmatiser'] = lemmatiser.lemmatize(word, pos=pos)\n",
    "    return normalised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d667b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Porter</th>\n",
       "      <th>Lancaster</th>\n",
       "      <th>Lemmatiser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pie</th>\n",
       "      <td>pie</td>\n",
       "      <td>pie</td>\n",
       "      <td>pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>globe</th>\n",
       "      <td>globe</td>\n",
       "      <td>glob</td>\n",
       "      <td>globe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>hous</td>\n",
       "      <td>hous</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knee</th>\n",
       "      <td>knee</td>\n",
       "      <td>kne</td>\n",
       "      <td>knee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle</th>\n",
       "      <td>angl</td>\n",
       "      <td>angl</td>\n",
       "      <td>angle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acetone</th>\n",
       "      <td>aceton</td>\n",
       "      <td>aceton</td>\n",
       "      <td>acetone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>time</td>\n",
       "      <td>tim</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brownie</th>\n",
       "      <td>browni</td>\n",
       "      <td>browny</td>\n",
       "      <td>brownie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>climat</td>\n",
       "      <td>clim</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>independence</th>\n",
       "      <td>independ</td>\n",
       "      <td>independ</td>\n",
       "      <td>independence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Porter Lancaster    Lemmatiser\n",
       "pie                pie       pie           pie\n",
       "globe            globe      glob         globe\n",
       "house             hous      hous         house\n",
       "knee              knee       kne          knee\n",
       "angle             angl      angl         angle\n",
       "acetone         aceton    aceton       acetone\n",
       "time              time       tim          time\n",
       "brownie         browni    browny       brownie\n",
       "climate         climat      clim       climate\n",
       "independence  independ  independ  independence"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise_text(['pie', 'globe', 'house', 'knee', 'angle', 'acetone', 'time', 'brownie', 'climate', 'independence'], pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4421980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Porter</th>\n",
       "      <th>Lancaster</th>\n",
       "      <th>Lemmatiser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wrote</th>\n",
       "      <td>wrote</td>\n",
       "      <td>wrot</td>\n",
       "      <td>write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinking</th>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remembered</th>\n",
       "      <td>rememb</td>\n",
       "      <td>rememb</td>\n",
       "      <td>remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relies</th>\n",
       "      <td>reli</td>\n",
       "      <td>rely</td>\n",
       "      <td>rely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>ate</td>\n",
       "      <td>at</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gone</th>\n",
       "      <td>gone</td>\n",
       "      <td>gon</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won</th>\n",
       "      <td>won</td>\n",
       "      <td>won</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swimming</th>\n",
       "      <td>swim</td>\n",
       "      <td>swim</td>\n",
       "      <td>swim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistreated</th>\n",
       "      <td>mistreat</td>\n",
       "      <td>mist</td>\n",
       "      <td>mistreat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Porter Lancaster Lemmatiser\n",
       "wrote          wrote      wrot      write\n",
       "thinking       think     think      think\n",
       "remembered    rememb    rememb   remember\n",
       "relies          reli      rely       rely\n",
       "ate              ate        at        eat\n",
       "gone            gone       gon         go\n",
       "won              won       won        win\n",
       "ran              ran       ran        run\n",
       "swimming        swim      swim       swim\n",
       "mistreated  mistreat      mist   mistreat"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise_text(['wrote', 'thinking', 'remembered', 'relies', 'ate', 'gone', 'won', 'ran', 'swimming', 'mistreated'], pos='v')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb25bb",
   "metadata": {},
   "source": [
    "# Speed Comparison\n",
    "When researching about lemmatisation and stemming, I have came across many resources stating that stemming is faster than lemmatisation. However, when I test three three normalisers on a sample data on my computer, I have observed quite the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c0394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a19cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "reviews = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    reviews.append((tag, movie_reviews.raw(fileid)))\n",
    "sample = pd.DataFrame(reviews, columns=['target', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab6fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\RAHUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "370aec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare one giant string \n",
    "sample_string = \" \".join(sample['document'].values)\n",
    "# Tokenise data\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokeniser.tokenize(sample_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def2fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.55 s Â± 30.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "[lemmatiser.lemmatize(token, 'v') for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba1aacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 s Â± 302 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "porter = PorterStemmer()\n",
    "[porter.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e737618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 s Â± 272 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "lancaster = LancasterStemmer()\n",
    "[lancaster.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aee7e4",
   "metadata": {},
   "source": [
    "As you can see, from this quick assessment, lemmatiser was in fact faster even when we compare a range with mean +/- 3 standard deviations. Lemmatiser therefore looks more favourable since it normalises sensibly and is faster to run. I will share 2 tips for effective lemmatisation as a bonus in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed82e43",
   "metadata": {},
   "source": [
    "5. Two tips for effective lemmatisation ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad727496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatising 'remembered' with pos='v' results in: remember\n",
      "Lemmatising 'remembered' with pos='n' results in: remembered\n",
      "\n",
      "Lemmatising 'universities' with pos='v' results in: universities\n",
      "Lemmatising 'universities' with pos='n' results in: university\n"
     ]
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "print(f\"Lemmatising 'remembered' with pos='v' results in: {lemmatiser.lemmatize('remembered', 'v')}\")\n",
    "print(f\"Lemmatising 'remembered' with pos='n' results in: {lemmatiser.lemmatize('remembered', 'n')}\\n\")\n",
    "print(f\"Lemmatising 'universities' with pos='v' results in: {lemmatiser.lemmatize('universities', 'v')}\")\n",
    "print(f\"Lemmatising 'universities' with pos='n' results in: {lemmatiser.lemmatize('universities', 'n')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33e6c6",
   "metadata": {},
   "source": [
    "As you can see, to effectively normalise words with WordNetLemmatizer, itâ€™s important to provide correct pos argument for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd21e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatising 'Remembered' with pos='v' results in: Remembered\n",
      "Lemmatising 'Remembered' with pos='n' results in: Remembered\n",
      "\n",
      "Lemmatising 'Universities' with pos='v' results in: Universities\n",
      "Lemmatising 'Universities' with pos='n' results in: Universities\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lemmatising 'Remembered' with pos='v' results in: {lemmatiser.lemmatize('Remembered', 'v')}\")\n",
    "print(f\"Lemmatising 'Remembered' with pos='n' results in: {lemmatiser.lemmatize('Remembered', 'n')}\\n\")\n",
    "print(f\"Lemmatising 'Universities' with pos='v' results in: {lemmatiser.lemmatize('Universities', 'v')}\")\n",
    "print(f\"Lemmatising 'Universities' with pos='n' results in: {lemmatiser.lemmatize('Universities', 'n')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32e7f6",
   "metadata": {},
   "source": [
    "When capitalised, words remain unchanged even with the correct pos because they are viewed as proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161048c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
